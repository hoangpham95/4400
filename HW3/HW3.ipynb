{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "matrix = []\n",
    "\n",
    "with open(\"spambase.data\", \"r\") as raw_data:\n",
    "    for raw_line in raw_data:\n",
    "        line = [float(x) for x in raw_line.split(\",\")]\n",
    "        matrix.append(line)\n",
    "\n",
    "data = pd.DataFrame(matrix)\n",
    "row, col = data.shape\n",
    "X, y = data.iloc[:,:col - 1], data[col - 1]\n",
    "# y = y.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/Documents/school/4400/env/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91, 0.09],\n",
       "       [0.38, 0.62],\n",
       "       [0.22, 0.78],\n",
       "       ...,\n",
       "       [0.92, 0.08],\n",
       "       [0.03, 0.97],\n",
       "       [0.99, 0.01]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classfierData(clf):\n",
    "    test_data = clf.predict(X_test)\n",
    "    print(\"Confusion matrix\\n\", confusion_matrix(test_data, y_test), \"\\n\")\n",
    "    tn, fp, fn, tp = confusion_matrix(test_data, y_test).ravel()\n",
    "    print(\"True negative:\", tn, \", false positive:\", fp, \", false negative:\", fn, \",true positive:\", tp, \"\\n\")\n",
    "    print(\"Accuracy score\", accuracy_score(test_data, y_test), \"\\n\")\n",
    "    print(\"Precision\", precision_score(test_data, y_test), \"\\n\")\n",
    "    print(\"Recall\", recall_score(test_data, y_test), \"\\n\")\n",
    "    print(\"F1 score\", f1_score(test_data, y_test), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      " [[678  34]\n",
      " [ 19 420]] \n",
      "\n",
      "True negative: 678 , false positive: 34 , false negative: 19 ,true positive: 420 \n",
      "\n",
      "Accuracy score 0.9539530842745438 \n",
      "\n",
      "Precision 0.9251101321585903 \n",
      "\n",
      "Recall 0.9567198177676538 \n",
      "\n",
      "F1 score 0.9406494960806271 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = clf.predict(X_test)\n",
    "\n",
    "print(\"Confusion matrix\\n\", confusion_matrix(test_data, y_test), \"\\n\")\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(test_data, y_test).ravel()\n",
    "print(\"True negative:\", tn, \", false positive:\", fp, \", false negative:\", fn, \",true positive:\", tp, \"\\n\")\n",
    "\n",
    "print(\"Accuracy score\", accuracy_score(test_data, y_test), \"\\n\")\n",
    "\n",
    "print(\"Precision\", precision_score(test_data, y_test), \"\\n\")\n",
    "\n",
    "print(\"Recall\", recall_score(test_data, y_test), \"\\n\")\n",
    "\n",
    "print(\"F1 score\", f1_score(test_data, y_test), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      " [[2090    1]\n",
      " [   1 1358]] \n",
      "\n",
      "True negative: 2090 , false positive: 1 , false negative: 1 ,true positive: 1358 \n",
      "\n",
      "Accuracy score 0.9994202898550725 \n",
      "\n",
      "Precision 0.9992641648270787 \n",
      "\n",
      "Recall 0.9992641648270787 \n",
      "\n",
      "F1 score 0.9992641648270787 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = clf.predict(X_train)\n",
    "\n",
    "print(\"Confusion matrix\\n\", confusion_matrix(test_data, y_train), \"\\n\")\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(test_data, y_train).ravel()\n",
    "print(\"True negative:\", tn, \", false positive:\", fp, \", false negative:\", fn, \",true positive:\", tp, \"\\n\")\n",
    "\n",
    "print(\"Accuracy score\", accuracy_score(test_data, y_train), \"\\n\")\n",
    "\n",
    "print(\"Precision\", precision_score(test_data, y_train), \"\\n\")\n",
    "\n",
    "print(\"Recall\", recall_score(test_data, y_train), \"\\n\")\n",
    "\n",
    "print(\"F1 score\", f1_score(test_data, y_train), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predicate:\n",
    "    def __init__(self, column, value):\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "    \n",
    "    def match(self, example, pp=False):\n",
    "        if pp:\n",
    "            print(example.shape, type(example))\n",
    "        val = example[self.column]\n",
    "        if Util.is_numeric(val):\n",
    "            return val >= self.value\n",
    "        else:\n",
    "            return val == self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Util:\n",
    "    @staticmethod\n",
    "    def label_count(labels):\n",
    "        count = {}\n",
    "        for r in labels:\n",
    "            if r not in count:\n",
    "                count[r] = 0\n",
    "            count[r] += 1\n",
    "        return count\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_numeric(val):\n",
    "        return isinstance(val, int) or isinstance(val, float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def partition(X, y, pred):\n",
    "        true_X, false_X, true_y, false_y = [], [], [], []\n",
    "        \n",
    "        for x_inst, y_inst in zip(X, y):\n",
    "            if pred.match(x_inst):\n",
    "                true_X.append(x_inst)\n",
    "                true_y.append(y_inst)\n",
    "            else:\n",
    "                false_X.append(x_inst)\n",
    "                false_y.append(y_inst)\n",
    "        return true_X, true_y, false_X, false_y\n",
    "    \n",
    "    @staticmethod\n",
    "    def gini_impur(labels):\n",
    "        \"\"\"\n",
    "        Gini impurity\n",
    "        \"\"\"\n",
    "        counts = Util.label_count(labels)\n",
    "        total = 0\n",
    "        for lbl in counts:\n",
    "            prob_of_lbl = float(counts[lbl]) / len(labels)\n",
    "            total += (prob_of_lbl * prob_of_lbl)\n",
    "        return 1 - total\n",
    "    \n",
    "    @staticmethod\n",
    "    def info_gain(leftLbl, rightLbl, curr_uncertainty):\n",
    "        \"\"\"\n",
    "        Calculating information gain\n",
    "        \"\"\"\n",
    "        p = float (len(leftLbl)) / (len(leftLbl) + len(rightLbl))\n",
    "        return curr_uncertainty - p * Util.gini_impur(leftLbl) - (1 - p) * Util.gini_impur(rightLbl)\n",
    "        \n",
    "class Leaf:\n",
    "    def __init__(self, y, depth=0):\n",
    "        print(\"Leaf at depth\", depth)\n",
    "        self.depth = depth\n",
    "        pred = Util.label_count(y)\n",
    "        for l in pred:\n",
    "            pred[l] = pred[l] / len(y)\n",
    "        self.predictions = pred\n",
    "        \n",
    "    def isLeaf(self):\n",
    "        return True\n",
    "\n",
    "class DecTreeNode:\n",
    "    def __init__(self, pred, true_branch, false_branch, depth=0):\n",
    "        self.depth = depth\n",
    "        self.pred = pred\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch\n",
    "        \n",
    "    def isLeaf(self):\n",
    "        return False\n",
    "    \n",
    "class DecTreeClassifier:\n",
    "    def __init__(self, max_depth=10):\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def findBestSplit(self, X, y):\n",
    "        start = time.time()\n",
    "        best_gain = 0  \n",
    "        best_pred = None\n",
    "        current_uncert = Util.gini_impur(y)\n",
    "        n_features = np.array(X).shape[1]\n",
    "        \n",
    "        for col in range(n_features):\n",
    "            vals = set([row[col] for row in X]) # different values in column\n",
    "            \n",
    "            for v in vals:\n",
    "                pred = Predicate(col, v)\n",
    "                true_X, true_y, false_X, false_y = Util.partition(X, y, pred)\n",
    "                \n",
    "                if len(true_X) == 0 or len(false_X) == 0:\n",
    "                    continue\n",
    "                \n",
    "                gain = Util.info_gain(true_y, false_y, current_uncert)\n",
    "                \n",
    "                if gain >= best_gain:\n",
    "                    best_gain, best_pred = gain, pred\n",
    "        print(\"Find best split size\", np.array(X).shape, \"took\", time.time() - start)\n",
    "        return best_gain, best_pred\n",
    "    \n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        gain, pred = self.findBestSplit(X, y)\n",
    "        if gain == 0 or depth >= self.max_depth:\n",
    "            return Leaf(y, depth)\n",
    "        true_X, true_y, false_X, false_y = Util.partition(X, y, pred)\n",
    "        \n",
    "        true_branch = self.build_tree(true_X, true_y, depth + 1)\n",
    "        false_branch = self.build_tree(false_X, false_y, depth + 1)\n",
    "        \n",
    "        return DecTreeNode(pred, true_branch, false_branch)\n",
    "    \n",
    "    def predict_by_tree(self, tree, X_inst):\n",
    "        if tree.isLeaf():\n",
    "            return tree.predictions\n",
    "        elif tree.pred.match(X_inst, pp=True):\n",
    "            return self.predict_by_tree(tree.true_branch, X_inst)\n",
    "        else:\n",
    "            return self.predict_by_tree(tree.false_branch, X_inst)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.build_tree(X.values, y.values) \n",
    "    \n",
    "    def predict(self, X_inst):\n",
    "        pred = self.predict_by_tree(self.tree, np.array(X_inst))\n",
    "        max_arg, max_prob = None, 0\n",
    "        for l in pred:\n",
    "            if pred[l] > max_prob:\n",
    "                max_arg = l\n",
    "            max_prob = max(max_prob, pred[l])\n",
    "        return max_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3450"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find best split size (3450, 57) took 54.23882532119751\n",
      "Find best split size (881, 57) took 6.615339279174805\n",
      "Find best split size (66, 57) took 0.11762785911560059\n",
      "Find best split size (6, 57) took 0.001325845718383789\n",
      "Leaf at depth 3\n",
      "Find best split size (60, 57) took 0.08471345901489258\n",
      "Leaf at depth 3\n",
      "Find best split size (815, 57) took 4.750385522842407\n",
      "Find best split size (12, 57) took 0.0025866031646728516\n",
      "Leaf at depth 3\n",
      "Find best split size (803, 57) took 4.63569188117981\n",
      "Find best split size (9, 57) took 0.0016345977783203125\n",
      "Leaf at depth 4\n",
      "Find best split size (794, 57) took 5.278500080108643\n",
      "Find best split size (8, 57) took 0.002809762954711914\n",
      "Find best split size (1, 57) took 0.0001800060272216797\n",
      "Leaf at depth 6\n",
      "Find best split size (7, 57) took 0.0021004676818847656\n",
      "Leaf at depth 6\n",
      "Find best split size (786, 57) took 4.622846364974976\n",
      "Find best split size (757, 57) took 4.600058555603027\n",
      "Find best split size (8, 57) took 0.0029668807983398438\n",
      "Find best split size (4, 57) took 0.0008978843688964844\n",
      "Leaf at depth 8\n",
      "Find best split size (4, 57) took 0.0008656978607177734\n",
      "Find best split size (3, 57) took 0.0005877017974853516\n",
      "Leaf at depth 9\n",
      "Find best split size (1, 57) took 0.0001735687255859375\n",
      "Leaf at depth 9\n",
      "Find best split size (749, 57) took 4.390413522720337\n",
      "Find best split size (1, 57) took 0.00015807151794433594\n",
      "Leaf at depth 8\n",
      "Find best split size (748, 57) took 4.579635143280029\n",
      "Find best split size (1, 57) took 0.000156402587890625\n",
      "Leaf at depth 9\n",
      "Find best split size (747, 57) took 4.086587905883789\n",
      "Find best split size (1, 57) took 0.00015807151794433594\n",
      "Leaf at depth 10\n",
      "Find best split size (746, 57) took 4.102853775024414\n",
      "Leaf at depth 10\n",
      "Find best split size (29, 57) took 0.011047124862670898\n",
      "Find best split size (7, 57) took 0.00106048583984375\n",
      "Find best split size (6, 57) took 0.0009007453918457031\n",
      "Leaf at depth 8\n",
      "Find best split size (1, 57) took 0.00015282630920410156\n",
      "Leaf at depth 8\n",
      "Find best split size (22, 57) took 0.008876323699951172\n",
      "Find best split size (3, 57) took 0.0004918575286865234\n",
      "Find best split size (2, 57) took 0.0002841949462890625\n",
      "Leaf at depth 9\n",
      "Find best split size (1, 57) took 0.00014257431030273438\n",
      "Leaf at depth 9\n",
      "Find best split size (19, 57) took 0.00611424446105957\n",
      "Find best split size (1, 57) took 0.00014257431030273438\n",
      "Leaf at depth 9\n",
      "Find best split size (18, 57) took 0.006013393402099609\n",
      "Leaf at depth 9\n",
      "Find best split size (2569, 57) took 26.761821508407593\n",
      "Find best split size (228, 57) took 0.6852867603302002\n",
      "Find best split size (11, 57) took 0.006601572036743164\n",
      "Leaf at depth 3\n",
      "Find best split size (217, 57) took 0.7158591747283936\n",
      "Find best split size (2, 57) took 0.0002593994140625\n",
      "Leaf at depth 4\n",
      "Find best split size (215, 57) took 0.5525140762329102\n",
      "Find best split size (5, 57) took 0.0011091232299804688\n",
      "Find best split size (3, 57) took 0.0005331039428710938\n",
      "Leaf at depth 6\n",
      "Find best split size (2, 57) took 0.0003476142883300781\n",
      "Leaf at depth 6\n",
      "Find best split size (210, 57) took 0.4316744804382324\n",
      "Find best split size (1, 57) took 0.00015306472778320312\n",
      "Leaf at depth 6\n",
      "Find best split size (209, 57) took 0.6423919200897217\n",
      "Find best split size (1, 57) took 0.00021266937255859375\n",
      "Leaf at depth 7\n",
      "Find best split size (208, 57) took 0.4533848762512207\n",
      "Find best split size (1, 57) took 0.00015234947204589844\n",
      "Leaf at depth 8\n",
      "Find best split size (207, 57) took 0.5540771484375\n",
      "Find best split size (2, 57) took 0.0003654956817626953\n",
      "Find best split size (1, 57) took 0.0001811981201171875\n",
      "Leaf at depth 10\n",
      "Find best split size (1, 57) took 0.00017118453979492188\n",
      "Leaf at depth 10\n",
      "Find best split size (205, 57) took 0.5297915935516357\n",
      "Find best split size (3, 57) took 0.0003561973571777344\n",
      "Leaf at depth 10\n",
      "Find best split size (202, 57) took 0.5949983596801758\n",
      "Leaf at depth 10\n",
      "Find best split size (2341, 57) took 22.555137634277344\n",
      "Find best split size (248, 57) took 0.3723475933074951\n",
      "Find best split size (150, 57) took 0.16947674751281738\n",
      "Find best split size (6, 57) took 0.0015590190887451172\n",
      "Leaf at depth 5\n",
      "Find best split size (144, 57) took 0.17574667930603027\n",
      "Find best split size (11, 57) took 0.0020318031311035156\n",
      "Find best split size (5, 57) took 0.0009591579437255859\n",
      "Leaf at depth 7\n",
      "Find best split size (6, 57) took 0.0010094642639160156\n",
      "Leaf at depth 7\n",
      "Find best split size (133, 57) took 0.13154387474060059\n",
      "Find best split size (3, 57) took 0.0003821849822998047\n",
      "Find best split size (1, 57) took 0.0001430511474609375\n",
      "Leaf at depth 8\n",
      "Find best split size (2, 57) took 0.00025081634521484375\n",
      "Leaf at depth 8\n",
      "Find best split size (130, 57) took 0.13724160194396973\n",
      "Find best split size (129, 57) took 0.1377260684967041\n",
      "Find best split size (1, 57) took 0.00017118453979492188\n",
      "Leaf at depth 9\n",
      "Find best split size (128, 57) took 0.12848377227783203\n",
      "Find best split size (123, 57) took 0.1151425838470459\n",
      "Leaf at depth 10\n",
      "Find best split size (5, 57) took 0.0008490085601806641\n",
      "Leaf at depth 10\n",
      "Find best split size (1, 57) took 0.00015115737915039062\n",
      "Leaf at depth 8\n",
      "Find best split size (98, 57) took 0.048383474349975586\n",
      "Find best split size (9, 57) took 0.0022161006927490234\n",
      "Find best split size (8, 57) took 0.001505136489868164\n",
      "Leaf at depth 6\n",
      "Find best split size (1, 57) took 0.0001513957977294922\n",
      "Leaf at depth 6\n",
      "Find best split size (89, 57) took 0.03905820846557617\n",
      "Find best split size (5, 57) took 0.001310586929321289\n",
      "Leaf at depth 6\n",
      "Find best split size (84, 57) took 0.03619551658630371\n",
      "Find best split size (5, 57) took 0.0005924701690673828\n",
      "Find best split size (4, 57) took 0.0006916522979736328\n",
      "Leaf at depth 8\n",
      "Find best split size (1, 57) took 0.00016832351684570312\n",
      "Leaf at depth 8\n",
      "Find best split size (79, 57) took 0.03403759002685547\n",
      "Find best split size (31, 57) took 0.006289005279541016\n",
      "Find best split size (12, 57) took 0.002295970916748047\n",
      "Find best split size (3, 57) took 0.0006856918334960938\n",
      "Leaf at depth 10\n",
      "Find best split size (9, 57) took 0.0019905567169189453\n",
      "Leaf at depth 10\n",
      "Find best split size (19, 57) took 0.0032427310943603516\n",
      "Find best split size (17, 57) took 0.003673553466796875\n",
      "Leaf at depth 10\n",
      "Find best split size (2, 57) took 0.00040721893310546875\n",
      "Leaf at depth 10\n",
      "Find best split size (48, 57) took 0.016244173049926758\n",
      "Find best split size (44, 57) took 0.011784791946411133\n",
      "Find best split size (8, 57) took 0.0010046958923339844\n",
      "Leaf at depth 10\n",
      "Find best split size (36, 57) took 0.009478569030761719\n",
      "Leaf at depth 10\n",
      "Find best split size (4, 57) took 0.0005042552947998047\n",
      "Find best split size (2, 57) took 0.00022459030151367188\n",
      "Leaf at depth 10\n",
      "Find best split size (2, 57) took 0.00021791458129882812\n",
      "Leaf at depth 10\n",
      "Find best split size (2093, 57) took 20.52314782142639\n",
      "Find best split size (179, 57) took 0.30521368980407715\n",
      "Find best split size (44, 57) took 0.024292945861816406\n",
      "Find best split size (26, 57) took 0.009021997451782227\n",
      "Find best split size (2, 57) took 0.00028443336486816406\n",
      "Leaf at depth 7\n",
      "Find best split size (24, 57) took 0.009049177169799805\n",
      "Leaf at depth 7\n",
      "Find best split size (18, 57) took 0.0059735774993896484\n",
      "Find best split size (4, 57) took 0.0006296634674072266\n",
      "Leaf at depth 7\n",
      "Find best split size (14, 57) took 0.004306793212890625\n",
      "Find best split size (7, 57) took 0.0015859603881835938\n",
      "Leaf at depth 8\n",
      "Find best split size (7, 57) took 0.0013091564178466797\n",
      "Find best split size (4, 57) took 0.0011749267578125\n",
      "Find best split size (3, 57) took 0.0005152225494384766\n",
      "Leaf at depth 10\n",
      "Find best split size (1, 57) took 0.0001513957977294922\n",
      "Leaf at depth 10\n",
      "Find best split size (3, 57) took 0.0018072128295898438\n",
      "Leaf at depth 9\n",
      "Find best split size (135, 57) took 0.17805743217468262\n",
      "Find best split size (26, 57) took 0.009736299514770508\n",
      "Find best split size (21, 57) took 0.0090484619140625\n",
      "Find best split size (2, 57) took 0.0010766983032226562\n",
      "Leaf at depth 8\n",
      "Find best split size (19, 57) took 0.006057262420654297\n",
      "Leaf at depth 8\n",
      "Find best split size (5, 57) took 0.0010280609130859375\n",
      "Leaf at depth 7\n",
      "Find best split size (109, 57) took 0.12555575370788574\n",
      "Find best split size (12, 57) took 0.0029582977294921875\n",
      "Find best split size (6, 57) took 0.0011706352233886719\n",
      "Leaf at depth 8\n",
      "Find best split size (6, 57) took 0.0022614002227783203\n",
      "Find best split size (3, 57) took 0.0010066032409667969\n",
      "Leaf at depth 9\n",
      "Find best split size (3, 57) took 0.0006539821624755859\n",
      "Find best split size (1, 57) took 0.0001914501190185547\n",
      "Leaf at depth 10\n",
      "Find best split size (2, 57) took 0.0003254413604736328\n",
      "Leaf at depth 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find best split size (97, 57) took 0.09850859642028809\n",
      "Find best split size (3, 57) took 0.0004572868347167969\n",
      "Leaf at depth 8\n",
      "Find best split size (94, 57) took 0.10211682319641113\n",
      "Find best split size (5, 57) took 0.0010199546813964844\n",
      "Find best split size (3, 57) took 0.0005621910095214844\n",
      "Leaf at depth 10\n",
      "Find best split size (2, 57) took 0.0003077983856201172\n",
      "Leaf at depth 10\n",
      "Find best split size (89, 57) took 0.08285641670227051\n",
      "Find best split size (3, 57) took 0.0005085468292236328\n",
      "Leaf at depth 10\n",
      "Find best split size (86, 57) took 0.08248305320739746\n",
      "Leaf at depth 10\n",
      "Find best split size (1914, 57) took 16.381088733673096\n",
      "Find best split size (41, 57) took 0.020244836807250977\n",
      "Find best split size (22, 57) took 0.0049936771392822266\n",
      "Find best split size (2, 57) took 0.0003361701965332031\n",
      "Leaf at depth 7\n",
      "Find best split size (20, 57) took 0.004882097244262695\n",
      "Leaf at depth 7\n",
      "Find best split size (19, 57) took 0.0074787139892578125\n",
      "Find best split size (3, 57) took 0.0007908344268798828\n",
      "Leaf at depth 7\n",
      "Find best split size (16, 57) took 0.005971431732177734\n",
      "Find best split size (2, 57) took 0.0003476142883300781\n",
      "Leaf at depth 8\n",
      "Find best split size (14, 57) took 0.00491642951965332\n",
      "Leaf at depth 8\n",
      "Find best split size (1873, 57) took 14.539954423904419\n",
      "Find best split size (30, 57) took 0.009284019470214844\n",
      "Find best split size (8, 57) took 0.000881195068359375\n",
      "Leaf at depth 7\n",
      "Find best split size (22, 57) took 0.00716400146484375\n",
      "Find best split size (3, 57) took 0.0004379749298095703\n",
      "Leaf at depth 8\n",
      "Find best split size (19, 57) took 0.0052967071533203125\n",
      "Find best split size (14, 57) took 0.0034193992614746094\n",
      "Find best split size (1, 57) took 0.00014281272888183594\n",
      "Leaf at depth 10\n",
      "Find best split size (13, 57) took 0.003520488739013672\n",
      "Leaf at depth 10\n",
      "Find best split size (5, 57) took 0.0009114742279052734\n",
      "Find best split size (3, 57) took 0.0009133815765380859\n",
      "Leaf at depth 10\n",
      "Find best split size (2, 57) took 0.0002720355987548828\n",
      "Leaf at depth 10\n",
      "Find best split size (1843, 57) took 15.189380168914795\n",
      "Find best split size (663, 57) took 3.5253615379333496\n",
      "Find best split size (1, 57) took 0.0001583099365234375\n",
      "Leaf at depth 8\n",
      "Find best split size (662, 57) took 3.5540430545806885\n",
      "Find best split size (3, 57) took 0.0008540153503417969\n",
      "Find best split size (2, 57) took 0.0003998279571533203\n",
      "Leaf at depth 10\n",
      "Find best split size (1, 57) took 0.00015282630920410156\n",
      "Leaf at depth 10\n",
      "Find best split size (659, 57) took 3.8919599056243896\n",
      "Find best split size (7, 57) took 0.0022726058959960938\n",
      "Leaf at depth 10\n",
      "Find best split size (652, 57) took 3.4405314922332764\n",
      "Leaf at depth 10\n",
      "Find best split size (1180, 57) took 5.255575895309448\n",
      "Find best split size (50, 57) took 0.04679059982299805\n",
      "Find best split size (17, 57) took 0.010067939758300781\n",
      "Find best split size (2, 57) took 0.0003781318664550781\n",
      "Leaf at depth 10\n",
      "Find best split size (15, 57) took 0.007929325103759766\n",
      "Leaf at depth 10\n",
      "Find best split size (33, 57) took 0.018639087677001953\n",
      "Find best split size (12, 57) took 0.0033235549926757812\n",
      "Leaf at depth 10\n",
      "Find best split size (21, 57) took 0.008577108383178711\n",
      "Leaf at depth 10\n",
      "Find best split size (1130, 57) took 4.539993524551392\n",
      "Find best split size (352, 57) took 0.8318014144897461\n",
      "Find best split size (94, 57) took 0.08165860176086426\n",
      "Leaf at depth 10\n",
      "Find best split size (258, 57) took 0.4517374038696289\n",
      "Leaf at depth 10\n",
      "Find best split size (778, 57) took 1.6756291389465332\n",
      "Find best split size (1, 57) took 0.000152587890625\n",
      "Leaf at depth 10\n",
      "Find best split size (777, 57) took 1.965574026107788\n",
      "Leaf at depth 10\n",
      "Decision tree classifier take 255.40394973754883 seconds to build tree\n"
     ]
    }
   ],
   "source": [
    "decTreeClf = DecTreeClassifier(max_depth=10)\n",
    "start = time.time()\n",
    "decTreeClf.fit(X_train, y_train)\n",
    "print(\"Decision tree classifier take\", time.time() - start, \"seconds to build tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decTreeClf.predict(X_test[0]), y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decTreeClf.predict(X_test[40]), y_test[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1151,)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "class RandomForestClassifier:\n",
    "    def __init__(self, num_est=10, max_depth=5):\n",
    "        self.num_est = num_est\n",
    "        self.max_depth = max_depth\n",
    "        self.forest = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        # Bagging\n",
    "        for i in range(self.num_est):\n",
    "            X_samp, y_samp = resample(X, y, replace=True, random_state=0)\n",
    "            clf = DecTreeClassifier(max_depth=self.max_depth)\n",
    "            print(\"Train estimator #\", i + 1)\n",
    "            clf.fit(X_samp, y_samp)\n",
    "            self.forest.append(clf)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        labels = {}\n",
    "        for clf in self.forest:\n",
    "            y = clf.predict(X)\n",
    "            if y not in labels:\n",
    "                labels[y] = 0\n",
    "            labels[y] += 1\n",
    "        \n",
    "        for l in labels:\n",
    "            labels[l] = labels[l] / self.num_est\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train estimator # 1\n",
      "Find best split size (500, 57) took 1.7294952869415283\n",
      "Find best split size (120, 57) took 0.12623858451843262\n",
      "Find best split size (115, 57) took 0.12290191650390625\n",
      "Find best split size (1, 57) took 0.0001785755157470703\n",
      "Leaf at depth 3\n",
      "Find best split size (114, 57) took 0.12784934043884277\n",
      "Find best split size (4, 57) took 0.0007719993591308594\n",
      "Find best split size (2, 57) took 0.00021028518676757812\n",
      "Leaf at depth 5\n",
      "Find best split size (2, 57) took 0.0002808570861816406\n",
      "Leaf at depth 5\n",
      "Find best split size (110, 57) took 0.11803388595581055\n",
      "Find best split size (15, 57) took 0.004353046417236328\n",
      "Leaf at depth 5\n",
      "Find best split size (95, 57) took 0.11424446105957031\n",
      "Leaf at depth 5\n",
      "Find best split size (5, 57) took 0.0008034706115722656\n",
      "Leaf at depth 2\n",
      "Find best split size (380, 57) took 0.7628130912780762\n",
      "Find best split size (68, 57) took 0.03416919708251953\n",
      "Find best split size (9, 57) took 0.001562356948852539\n",
      "Leaf at depth 3\n",
      "Find best split size (59, 57) took 0.028524398803710938\n",
      "Find best split size (8, 57) took 0.0014009475708007812\n",
      "Find best split size (2, 57) took 0.00021147727966308594\n",
      "Leaf at depth 5\n",
      "Find best split size (6, 57) took 0.0014736652374267578\n",
      "Leaf at depth 5\n",
      "Find best split size (51, 57) took 0.022020816802978516\n",
      "Find best split size (4, 57) took 0.0008494853973388672\n",
      "Leaf at depth 5\n",
      "Find best split size (47, 57) took 0.01842331886291504\n",
      "Leaf at depth 5\n",
      "Find best split size (312, 57) took 0.5095853805541992\n",
      "Find best split size (13, 57) took 0.002458333969116211\n",
      "Find best split size (2, 57) took 0.00034165382385253906\n",
      "Leaf at depth 4\n",
      "Find best split size (11, 57) took 0.0023021697998046875\n",
      "Leaf at depth 4\n",
      "Find best split size (299, 57) took 0.5254316329956055\n",
      "Find best split size (8, 57) took 0.0007865428924560547\n",
      "Leaf at depth 4\n",
      "Find best split size (291, 57) took 0.44260573387145996\n",
      "Find best split size (4, 57) took 0.0003085136413574219\n",
      "Leaf at depth 5\n",
      "Find best split size (287, 57) took 0.478879451751709\n",
      "Leaf at depth 5\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(num_est=1)\n",
    "rfc.fit(X_train[:500], y_train[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n",
      "(1151,) <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({0.0: 1.0}, 0.0)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.predict(X_test[50]), np.array(y_test)[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1151,)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[56].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=10), n_estimators=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classfierData(clf):\n",
    "    test_data = clf.predict(X_test)\n",
    "    print(\"Confusion matrix\\n\", confusion_matrix(test_data, y_test), \"\\n\")\n",
    "    tn, fp, fn, tp = confusion_matrix(test_data, y_test).ravel()\n",
    "    print(\"True negative:\", tn, \", false positive:\", fp, \", false negative:\", fn, \",true positive:\", tp, \"\\n\")\n",
    "    print(\"Accuracy score\", accuracy_score(test_data, y_test), \"\\n\")\n",
    "    print(\"Precision\", precision_score(test_data, y_test), \"\\n\")\n",
    "    print(\"Recall\", recall_score(test_data, y_test), \"\\n\")\n",
    "    print(\"F1 score\", f1_score(test_data, y_test), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classfierData(ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "num_classes = 10\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "import keras.callbacks as cb\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 28, 28))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print('Loading data...')\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "\n",
    "    y_train = np_utils.to_categorical(y_train, 10)\n",
    "    y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "    X_train = np.reshape(X_train, (60000, 784))\n",
    "    X_test = np.reshape(X_test, (10000, 784))\n",
    "\n",
    "    print('Data loaded.')\n",
    "    return [X_train, X_test, y_train, y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    start_time = time.time()\n",
    "    print('Compiling Model ... ')\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, input_dim=28*28))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(300))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    rms = RMSprop()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])\n",
    "    print('Model compield in {0} seconds'.format(time.time() - start_time))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(cb.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        batch_loss = logs.get('loss')\n",
    "        self.losses.append(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_network(data=None, model=None, epochs=20, batch=256):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        if data is None:\n",
    "            X_train, X_test, y_train, y_test = load_data()\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = data\n",
    "\n",
    "        if model is None:\n",
    "            model = init_model()\n",
    "\n",
    "        history = LossHistory()\n",
    "\n",
    "        print('Training model...')\n",
    "        model.fit(X_train, y_train, nb_epoch=epochs, batch_size=batch,\n",
    "                  callbacks=[history],\n",
    "                  validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "        print(\"Training duration : {0}\".format(time.time() - start_time))\n",
    "        score = model.evaluate(X_test, y_test, batch_size=16)\n",
    "\n",
    "        print(\"Network's test score [loss, accuracy]: {0}\".format(score))\n",
    "        return model, history.losses\n",
    "    except KeyboardInterrupt:\n",
    "        print('=== KeyboardInterrupt')\n",
    "        return model, history.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded.\n",
      "Compiling Model ... \n",
      "Model compield in 0.07654333114624023 seconds\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/Documents/school/4400/env/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 6s - loss: 0.3582 - acc: 0.8894 - val_loss: 0.1359 - val_acc: 0.9559\n",
      "Epoch 2/20\n",
      " - 5s - loss: 0.1550 - acc: 0.9537 - val_loss: 0.1141 - val_acc: 0.9643\n",
      "Epoch 3/20\n",
      " - 5s - loss: 0.1138 - acc: 0.9664 - val_loss: 0.0914 - val_acc: 0.9733\n",
      "Epoch 4/20\n",
      " - 4s - loss: 0.0926 - acc: 0.9718 - val_loss: 0.0791 - val_acc: 0.9779\n",
      "Epoch 5/20\n",
      " - 4s - loss: 0.0790 - acc: 0.9765 - val_loss: 0.0763 - val_acc: 0.9781\n",
      "Epoch 6/20\n",
      " - 4s - loss: 0.0717 - acc: 0.9784 - val_loss: 0.0705 - val_acc: 0.9802\n",
      "Epoch 7/20\n",
      " - 4s - loss: 0.0647 - acc: 0.9799 - val_loss: 0.0631 - val_acc: 0.9813\n",
      "Epoch 8/20\n",
      " - 4s - loss: 0.0572 - acc: 0.9823 - val_loss: 0.0640 - val_acc: 0.9831\n",
      "Epoch 9/20\n",
      " - 4s - loss: 0.0542 - acc: 0.9833 - val_loss: 0.0711 - val_acc: 0.9809\n",
      "Epoch 10/20\n",
      " - 4s - loss: 0.0511 - acc: 0.9845 - val_loss: 0.0755 - val_acc: 0.9819\n",
      "Epoch 11/20\n",
      " - 4s - loss: 0.0453 - acc: 0.9860 - val_loss: 0.0741 - val_acc: 0.9828\n",
      "Epoch 12/20\n",
      " - 4s - loss: 0.0446 - acc: 0.9865 - val_loss: 0.0747 - val_acc: 0.9822\n",
      "Epoch 13/20\n",
      " - 4s - loss: 0.0416 - acc: 0.9877 - val_loss: 0.0752 - val_acc: 0.9830\n",
      "Epoch 14/20\n",
      " - 4s - loss: 0.0397 - acc: 0.9881 - val_loss: 0.0824 - val_acc: 0.9805\n",
      "Epoch 15/20\n",
      " - 5s - loss: 0.0379 - acc: 0.9887 - val_loss: 0.0721 - val_acc: 0.9835\n",
      "Epoch 16/20\n",
      " - 5s - loss: 0.0348 - acc: 0.9894 - val_loss: 0.0763 - val_acc: 0.9835\n",
      "Epoch 17/20\n",
      " - 5s - loss: 0.0361 - acc: 0.9894 - val_loss: 0.0751 - val_acc: 0.9828\n",
      "Epoch 18/20\n",
      " - 4s - loss: 0.0334 - acc: 0.9905 - val_loss: 0.0838 - val_acc: 0.9825\n",
      "Epoch 19/20\n",
      " - 4s - loss: 0.0318 - acc: 0.9905 - val_loss: 0.0699 - val_acc: 0.9849\n",
      "Epoch 20/20\n",
      " - 4s - loss: 0.0330 - acc: 0.9901 - val_loss: 0.0764 - val_acc: 0.9847\n",
      "Training duration : 92.38692569732666\n",
      "10000/10000 [==============================] - 1s 106us/step\n",
      "Network's test score [loss, accuracy]: [0.07642816025055074, 0.9847]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.engine.sequential.Sequential at 0x7f777689dda0>,\n",
       " [2.3608015,\n",
       "  2.00507,\n",
       "  1.8083081,\n",
       "  1.3956115,\n",
       "  1.2315018,\n",
       "  1.0465443,\n",
       "  1.0262419,\n",
       "  0.89224064,\n",
       "  0.8475828,\n",
       "  1.047065,\n",
       "  0.7375021,\n",
       "  0.8289175,\n",
       "  0.85706085,\n",
       "  0.77104735,\n",
       "  0.6333288,\n",
       "  0.72992194,\n",
       "  0.5760086,\n",
       "  0.6427897,\n",
       "  0.53049195,\n",
       "  0.6990065,\n",
       "  0.63653266,\n",
       "  0.55504483,\n",
       "  0.53978693,\n",
       "  0.5734676,\n",
       "  0.57175136,\n",
       "  0.4985174,\n",
       "  0.4593255,\n",
       "  0.55867255,\n",
       "  0.65003383,\n",
       "  0.5271794,\n",
       "  0.4729493,\n",
       "  0.4595414,\n",
       "  0.48511487,\n",
       "  0.54606843,\n",
       "  0.3715879,\n",
       "  0.4201899,\n",
       "  0.45268357,\n",
       "  0.3985822,\n",
       "  0.4112661,\n",
       "  0.42359927,\n",
       "  0.6172523,\n",
       "  0.40121466,\n",
       "  0.29989642,\n",
       "  0.35638666,\n",
       "  0.41480958,\n",
       "  0.40489542,\n",
       "  0.45162535,\n",
       "  0.47289056,\n",
       "  0.4918967,\n",
       "  0.52513915,\n",
       "  0.44447148,\n",
       "  0.27197117,\n",
       "  0.3801303,\n",
       "  0.43973237,\n",
       "  0.36336458,\n",
       "  0.421684,\n",
       "  0.36935103,\n",
       "  0.29918665,\n",
       "  0.41233864,\n",
       "  0.32812318,\n",
       "  0.28132978,\n",
       "  0.3524776,\n",
       "  0.3783305,\n",
       "  0.4148354,\n",
       "  0.38574588,\n",
       "  0.3388809,\n",
       "  0.3842064,\n",
       "  0.4777791,\n",
       "  0.38009903,\n",
       "  0.3045026,\n",
       "  0.29592094,\n",
       "  0.18672818,\n",
       "  0.39913154,\n",
       "  0.28699875,\n",
       "  0.3551426,\n",
       "  0.28679135,\n",
       "  0.35300982,\n",
       "  0.42508584,\n",
       "  0.26102084,\n",
       "  0.4188611,\n",
       "  0.32749003,\n",
       "  0.32543603,\n",
       "  0.26565254,\n",
       "  0.36147875,\n",
       "  0.40103993,\n",
       "  0.29878494,\n",
       "  0.3882735,\n",
       "  0.28395194,\n",
       "  0.42664683,\n",
       "  0.30302364,\n",
       "  0.27957508,\n",
       "  0.2823121,\n",
       "  0.28025392,\n",
       "  0.23600733,\n",
       "  0.26348987,\n",
       "  0.25128144,\n",
       "  0.32719535,\n",
       "  0.2860173,\n",
       "  0.25341713,\n",
       "  0.3532404,\n",
       "  0.27779666,\n",
       "  0.18012458,\n",
       "  0.2736617,\n",
       "  0.32157737,\n",
       "  0.2809481,\n",
       "  0.22563213,\n",
       "  0.25908363,\n",
       "  0.2751022,\n",
       "  0.28020144,\n",
       "  0.2572062,\n",
       "  0.3149424,\n",
       "  0.2348917,\n",
       "  0.2848116,\n",
       "  0.24454165,\n",
       "  0.19747528,\n",
       "  0.2949163,\n",
       "  0.2354864,\n",
       "  0.20140973,\n",
       "  0.22438028,\n",
       "  0.27241403,\n",
       "  0.29038972,\n",
       "  0.2610786,\n",
       "  0.33406863,\n",
       "  0.3152932,\n",
       "  0.25962055,\n",
       "  0.27672625,\n",
       "  0.2792989,\n",
       "  0.2739143,\n",
       "  0.31986788,\n",
       "  0.2043643,\n",
       "  0.37704808,\n",
       "  0.19893269,\n",
       "  0.23714441,\n",
       "  0.28290915,\n",
       "  0.27354494,\n",
       "  0.269231,\n",
       "  0.20160843,\n",
       "  0.31664932,\n",
       "  0.23851894,\n",
       "  0.2588425,\n",
       "  0.1872832,\n",
       "  0.2071389,\n",
       "  0.30728376,\n",
       "  0.2568668,\n",
       "  0.20307136,\n",
       "  0.23199792,\n",
       "  0.19511363,\n",
       "  0.22093183,\n",
       "  0.27016574,\n",
       "  0.2787146,\n",
       "  0.20177999,\n",
       "  0.26899663,\n",
       "  0.2834953,\n",
       "  0.2694744,\n",
       "  0.19287892,\n",
       "  0.24060512,\n",
       "  0.2079656,\n",
       "  0.24777916,\n",
       "  0.23845573,\n",
       "  0.2646637,\n",
       "  0.35061514,\n",
       "  0.24954888,\n",
       "  0.22281519,\n",
       "  0.21286173,\n",
       "  0.19584598,\n",
       "  0.17983776,\n",
       "  0.25040329,\n",
       "  0.2847829,\n",
       "  0.24422818,\n",
       "  0.22955856,\n",
       "  0.19492093,\n",
       "  0.24565673,\n",
       "  0.17124838,\n",
       "  0.19058424,\n",
       "  0.22732893,\n",
       "  0.13789386,\n",
       "  0.22644985,\n",
       "  0.20092681,\n",
       "  0.1906589,\n",
       "  0.23031299,\n",
       "  0.16218525,\n",
       "  0.18223268,\n",
       "  0.24576576,\n",
       "  0.23155293,\n",
       "  0.3373307,\n",
       "  0.31907988,\n",
       "  0.24391352,\n",
       "  0.20766123,\n",
       "  0.22668281,\n",
       "  0.21176937,\n",
       "  0.2103945,\n",
       "  0.26875794,\n",
       "  0.21032645,\n",
       "  0.1944425,\n",
       "  0.14563315,\n",
       "  0.23421729,\n",
       "  0.28034955,\n",
       "  0.249821,\n",
       "  0.20989676,\n",
       "  0.30019712,\n",
       "  0.24628624,\n",
       "  0.23533854,\n",
       "  0.31587255,\n",
       "  0.19860502,\n",
       "  0.21429566,\n",
       "  0.18377998,\n",
       "  0.18001674,\n",
       "  0.19706677,\n",
       "  0.22148713,\n",
       "  0.14890662,\n",
       "  0.2352306,\n",
       "  0.2510077,\n",
       "  0.20585804,\n",
       "  0.28084874,\n",
       "  0.16994527,\n",
       "  0.24188682,\n",
       "  0.12734285,\n",
       "  0.28822365,\n",
       "  0.22995475,\n",
       "  0.22697668,\n",
       "  0.12581873,\n",
       "  0.19287646,\n",
       "  0.15779267,\n",
       "  0.18864045,\n",
       "  0.24525303,\n",
       "  0.28471315,\n",
       "  0.2741947,\n",
       "  0.16847178,\n",
       "  0.12956658,\n",
       "  0.18560243,\n",
       "  0.24267644,\n",
       "  0.23312685,\n",
       "  0.28660563,\n",
       "  0.25779003,\n",
       "  0.2530576,\n",
       "  0.198019,\n",
       "  0.19089681,\n",
       "  0.13926291,\n",
       "  0.149816,\n",
       "  0.20357558,\n",
       "  0.15251434,\n",
       "  0.23540625,\n",
       "  0.19702783,\n",
       "  0.17252211,\n",
       "  0.15798739,\n",
       "  0.16174316,\n",
       "  0.15372017,\n",
       "  0.14386475,\n",
       "  0.14533229,\n",
       "  0.17036614,\n",
       "  0.22846685,\n",
       "  0.12389153,\n",
       "  0.13934714,\n",
       "  0.24485502,\n",
       "  0.14828151,\n",
       "  0.20209785,\n",
       "  0.16884214,\n",
       "  0.16876404,\n",
       "  0.11445955,\n",
       "  0.10220459,\n",
       "  0.12525998,\n",
       "  0.119017676,\n",
       "  0.13500148,\n",
       "  0.21775201,\n",
       "  0.17936309,\n",
       "  0.1351441,\n",
       "  0.14218779,\n",
       "  0.14324759,\n",
       "  0.23765549,\n",
       "  0.21187687,\n",
       "  0.16047734,\n",
       "  0.14415453,\n",
       "  0.20776844,\n",
       "  0.15305799,\n",
       "  0.18194412,\n",
       "  0.13661073,\n",
       "  0.19477803,\n",
       "  0.2590065,\n",
       "  0.15925023,\n",
       "  0.12635349,\n",
       "  0.1569483,\n",
       "  0.18294549,\n",
       "  0.1156896,\n",
       "  0.16096586,\n",
       "  0.1833665,\n",
       "  0.18322733,\n",
       "  0.1428205,\n",
       "  0.16722187,\n",
       "  0.13984907,\n",
       "  0.23523226,\n",
       "  0.18740565,\n",
       "  0.2131336,\n",
       "  0.18905436,\n",
       "  0.16222897,\n",
       "  0.15116876,\n",
       "  0.17521992,\n",
       "  0.15533024,\n",
       "  0.13578966,\n",
       "  0.21615066,\n",
       "  0.16221228,\n",
       "  0.15515232,\n",
       "  0.12797958,\n",
       "  0.13399348,\n",
       "  0.14738256,\n",
       "  0.110379,\n",
       "  0.19400075,\n",
       "  0.22674482,\n",
       "  0.15330213,\n",
       "  0.14768147,\n",
       "  0.19284055,\n",
       "  0.17108554,\n",
       "  0.09207649,\n",
       "  0.09183134,\n",
       "  0.1938351,\n",
       "  0.21194574,\n",
       "  0.185796,\n",
       "  0.20521668,\n",
       "  0.14858925,\n",
       "  0.16135246,\n",
       "  0.18328482,\n",
       "  0.22630101,\n",
       "  0.14335512,\n",
       "  0.13026553,\n",
       "  0.14161173,\n",
       "  0.1385951,\n",
       "  0.18891257,\n",
       "  0.117461175,\n",
       "  0.15278943,\n",
       "  0.1651447,\n",
       "  0.1734722,\n",
       "  0.1099692,\n",
       "  0.17864886,\n",
       "  0.13613836,\n",
       "  0.13698713,\n",
       "  0.16326708,\n",
       "  0.156404,\n",
       "  0.23278764,\n",
       "  0.13154396,\n",
       "  0.15165606,\n",
       "  0.13074125,\n",
       "  0.18781662,\n",
       "  0.1754241,\n",
       "  0.1192104,\n",
       "  0.16847824,\n",
       "  0.1573343,\n",
       "  0.17882389,\n",
       "  0.12557262,\n",
       "  0.15061727,\n",
       "  0.17393216,\n",
       "  0.13884673,\n",
       "  0.110035464,\n",
       "  0.19393155,\n",
       "  0.2683792,\n",
       "  0.13000214,\n",
       "  0.17083187,\n",
       "  0.14871404,\n",
       "  0.23319611,\n",
       "  0.14248967,\n",
       "  0.11763483,\n",
       "  0.1587543,\n",
       "  0.25800708,\n",
       "  0.23556864,\n",
       "  0.22645131,\n",
       "  0.15750098,\n",
       "  0.14674677,\n",
       "  0.17365852,\n",
       "  0.15913625,\n",
       "  0.270264,\n",
       "  0.12431167,\n",
       "  0.12798968,\n",
       "  0.14799812,\n",
       "  0.106366605,\n",
       "  0.16293561,\n",
       "  0.12338537,\n",
       "  0.15732774,\n",
       "  0.10134432,\n",
       "  0.15850332,\n",
       "  0.14504954,\n",
       "  0.11231476,\n",
       "  0.14082307,\n",
       "  0.11359851,\n",
       "  0.11305047,\n",
       "  0.09025919,\n",
       "  0.15592375,\n",
       "  0.18732134,\n",
       "  0.14495642,\n",
       "  0.12623459,\n",
       "  0.10033582,\n",
       "  0.14723524,\n",
       "  0.17438117,\n",
       "  0.19102694,\n",
       "  0.15434237,\n",
       "  0.2540707,\n",
       "  0.24299783,\n",
       "  0.13726234,\n",
       "  0.14624095,\n",
       "  0.19557668,\n",
       "  0.093440235,\n",
       "  0.19836642,\n",
       "  0.18940912,\n",
       "  0.15888052,\n",
       "  0.19171679,\n",
       "  0.2599321,\n",
       "  0.12479671,\n",
       "  0.13917035,\n",
       "  0.13529834,\n",
       "  0.1048491,\n",
       "  0.10302463,\n",
       "  0.17373085,\n",
       "  0.15315646,\n",
       "  0.10143483,\n",
       "  0.16320592,\n",
       "  0.2036309,\n",
       "  0.13249725,\n",
       "  0.12498343,\n",
       "  0.17534629,\n",
       "  0.14055392,\n",
       "  0.091835305,\n",
       "  0.21002415,\n",
       "  0.10801538,\n",
       "  0.10839823,\n",
       "  0.09676419,\n",
       "  0.130712,\n",
       "  0.15799299,\n",
       "  0.105775155,\n",
       "  0.104798265,\n",
       "  0.09137223,\n",
       "  0.15664059,\n",
       "  0.11764224,\n",
       "  0.1694929,\n",
       "  0.2597202,\n",
       "  0.109768435,\n",
       "  0.107641846,\n",
       "  0.087449074,\n",
       "  0.10407883,\n",
       "  0.1375198,\n",
       "  0.18311428,\n",
       "  0.13860103,\n",
       "  0.14626046,\n",
       "  0.10830909,\n",
       "  0.18291704,\n",
       "  0.17895158,\n",
       "  0.120256856,\n",
       "  0.14694226,\n",
       "  0.05484096,\n",
       "  0.1391063,\n",
       "  0.14276308,\n",
       "  0.078197055,\n",
       "  0.10685717,\n",
       "  0.13048503,\n",
       "  0.14738423,\n",
       "  0.09535513,\n",
       "  0.1779953,\n",
       "  0.16706216,\n",
       "  0.14212547,\n",
       "  0.10595321,\n",
       "  0.188588,\n",
       "  0.09941301,\n",
       "  0.11321606,\n",
       "  0.15543549,\n",
       "  0.11717975,\n",
       "  0.10916048,\n",
       "  0.12335352,\n",
       "  0.11705026,\n",
       "  0.1971858,\n",
       "  0.08681059,\n",
       "  0.064429544,\n",
       "  0.09969784,\n",
       "  0.10493777,\n",
       "  0.123413235,\n",
       "  0.15177897,\n",
       "  0.14171192,\n",
       "  0.14157398,\n",
       "  0.15282795,\n",
       "  0.096728496,\n",
       "  0.08027486,\n",
       "  0.15162459,\n",
       "  0.11047268,\n",
       "  0.064878434,\n",
       "  0.1566436,\n",
       "  0.16382535,\n",
       "  0.14708616,\n",
       "  0.10189451,\n",
       "  0.09570612,\n",
       "  0.12620932,\n",
       "  0.044041734,\n",
       "  0.12373437,\n",
       "  0.10643551,\n",
       "  0.14459741,\n",
       "  0.1482457,\n",
       "  0.08641052,\n",
       "  0.09580708,\n",
       "  0.12355975,\n",
       "  0.13836133,\n",
       "  0.104503624,\n",
       "  0.08312979,\n",
       "  0.07110002,\n",
       "  0.11857101,\n",
       "  0.12086056,\n",
       "  0.16762996,\n",
       "  0.09982969,\n",
       "  0.15682226,\n",
       "  0.14421996,\n",
       "  0.075115845,\n",
       "  0.16359009,\n",
       "  0.048894867,\n",
       "  0.10426451,\n",
       "  0.088862956,\n",
       "  0.10994341,\n",
       "  0.09788206,\n",
       "  0.16116968,\n",
       "  0.08706698,\n",
       "  0.12214452,\n",
       "  0.12677833,\n",
       "  0.15355393,\n",
       "  0.09187325,\n",
       "  0.07871845,\n",
       "  0.12796412,\n",
       "  0.069948085,\n",
       "  0.10441835,\n",
       "  0.09954961,\n",
       "  0.099267334,\n",
       "  0.12721765,\n",
       "  0.06834311,\n",
       "  0.16911379,\n",
       "  0.13722059,\n",
       "  0.093226016,\n",
       "  0.05379831,\n",
       "  0.09550425,\n",
       "  0.09457691,\n",
       "  0.19633882,\n",
       "  0.115400374,\n",
       "  0.059912033,\n",
       "  0.08325202,\n",
       "  0.14866106,\n",
       "  0.14657673,\n",
       "  0.12097205,\n",
       "  0.10294914,\n",
       "  0.07663633,\n",
       "  0.083647415,\n",
       "  0.18941718,\n",
       "  0.07959789,\n",
       "  0.07717768,\n",
       "  0.09033184,\n",
       "  0.065288596,\n",
       "  0.11320414,\n",
       "  0.09773876,\n",
       "  0.15968496,\n",
       "  0.099667355,\n",
       "  0.18419732,\n",
       "  0.09149024,\n",
       "  0.06788556,\n",
       "  0.13147634,\n",
       "  0.09218913,\n",
       "  0.06688732,\n",
       "  0.07266587,\n",
       "  0.13631697,\n",
       "  0.07753476,\n",
       "  0.1691429,\n",
       "  0.09801684,\n",
       "  0.09110843,\n",
       "  0.07344563,\n",
       "  0.16331515,\n",
       "  0.17600234,\n",
       "  0.11461443,\n",
       "  0.068685465,\n",
       "  0.09711698,\n",
       "  0.08622009,\n",
       "  0.20434543,\n",
       "  0.13968965,\n",
       "  0.059483394,\n",
       "  0.15633793,\n",
       "  0.059804074,\n",
       "  0.14206249,\n",
       "  0.17002681,\n",
       "  0.1561682,\n",
       "  0.12332791,\n",
       "  0.097227976,\n",
       "  0.15107097,\n",
       "  0.12074904,\n",
       "  0.11751248,\n",
       "  0.07728466,\n",
       "  0.12830055,\n",
       "  0.15950409,\n",
       "  0.110527515,\n",
       "  0.16041043,\n",
       "  0.09654692,\n",
       "  0.09837469,\n",
       "  0.14926311,\n",
       "  0.12964864,\n",
       "  0.125962,\n",
       "  0.22207077,\n",
       "  0.14296925,\n",
       "  0.13094056,\n",
       "  0.105261885,\n",
       "  0.15100369,\n",
       "  0.06355953,\n",
       "  0.12531564,\n",
       "  0.114791244,\n",
       "  0.13023117,\n",
       "  0.10984469,\n",
       "  0.1310004,\n",
       "  0.16821107,\n",
       "  0.103088684,\n",
       "  0.07765814,\n",
       "  0.090855554,\n",
       "  0.07786639,\n",
       "  0.12376955,\n",
       "  0.114724174,\n",
       "  0.10034133,\n",
       "  0.17681292,\n",
       "  0.07717375,\n",
       "  0.12706852,\n",
       "  0.08145012,\n",
       "  0.076553345,\n",
       "  0.083147705,\n",
       "  0.09638822,\n",
       "  0.07301101,\n",
       "  0.06049299,\n",
       "  0.1429774,\n",
       "  0.09219842,\n",
       "  0.051798746,\n",
       "  0.15116799,\n",
       "  0.105642356,\n",
       "  0.09327483,\n",
       "  0.11333333,\n",
       "  0.09268142,\n",
       "  0.17893763,\n",
       "  0.10531126,\n",
       "  0.09099256,\n",
       "  0.07474772,\n",
       "  0.12536593,\n",
       "  0.11221019,\n",
       "  0.14629307,\n",
       "  0.08312589,\n",
       "  0.119509086,\n",
       "  0.100701466,\n",
       "  0.17199463,\n",
       "  0.08623429,\n",
       "  0.1552212,\n",
       "  0.101767026,\n",
       "  0.15439212,\n",
       "  0.12327867,\n",
       "  0.14298117,\n",
       "  0.08969875,\n",
       "  0.20816152,\n",
       "  0.08942067,\n",
       "  0.0975091,\n",
       "  0.061426766,\n",
       "  0.06071903,\n",
       "  0.14462842,\n",
       "  0.11039235,\n",
       "  0.20140247,\n",
       "  0.15032323,\n",
       "  0.094262004,\n",
       "  0.06052553,\n",
       "  0.09927124,\n",
       "  0.070843376,\n",
       "  0.08661319,\n",
       "  0.07604991,\n",
       "  0.09919442,\n",
       "  0.10657669,\n",
       "  0.13315842,\n",
       "  0.08323486,\n",
       "  0.06361676,\n",
       "  0.11381159,\n",
       "  0.0858646,\n",
       "  0.10079791,\n",
       "  0.09596219,\n",
       "  0.1085413,\n",
       "  0.1217525,\n",
       "  0.1753627,\n",
       "  0.16644312,\n",
       "  0.081322566,\n",
       "  0.11636006,\n",
       "  0.0794623,\n",
       "  0.05639074,\n",
       "  0.091580585,\n",
       "  0.07949813,\n",
       "  0.12708057,\n",
       "  0.09758636,\n",
       "  0.16377059,\n",
       "  0.101774804,\n",
       "  0.19863442,\n",
       "  0.09226773,\n",
       "  0.082454026,\n",
       "  0.099730074,\n",
       "  0.14481714,\n",
       "  0.12013024,\n",
       "  0.11294215,\n",
       "  0.12690827,\n",
       "  0.12303723,\n",
       "  0.085270055,\n",
       "  0.14556737,\n",
       "  0.119852364,\n",
       "  0.083254084,\n",
       "  0.08038877,\n",
       "  0.12662971,\n",
       "  0.12347707,\n",
       "  0.106469385,\n",
       "  0.07770415,\n",
       "  0.15999235,\n",
       "  0.10902056,\n",
       "  0.15995936,\n",
       "  0.22849123,\n",
       "  0.10398798,\n",
       "  0.10420933,\n",
       "  0.088449925,\n",
       "  0.084009916,\n",
       "  0.04885038,\n",
       "  0.10259302,\n",
       "  0.16989107,\n",
       "  0.07234612,\n",
       "  0.08771981,\n",
       "  0.08205016,\n",
       "  0.071257606,\n",
       "  0.118632734,\n",
       "  0.059083253,\n",
       "  0.09493686,\n",
       "  0.12933029,\n",
       "  0.06998109,\n",
       "  0.117302656,\n",
       "  0.036089975,\n",
       "  0.17265025,\n",
       "  0.09397512,\n",
       "  0.06330375,\n",
       "  0.088926554,\n",
       "  0.10859243,\n",
       "  0.042498827,\n",
       "  0.13255616,\n",
       "  0.09127539,\n",
       "  0.07810214,\n",
       "  0.10141422,\n",
       "  0.073064454,\n",
       "  0.05344908,\n",
       "  0.06291506,\n",
       "  0.090781614,\n",
       "  0.084612235,\n",
       "  0.116062626,\n",
       "  0.0816192,\n",
       "  0.081692755,\n",
       "  0.12054971,\n",
       "  0.058783848,\n",
       "  0.06967938,\n",
       "  0.15187955,\n",
       "  0.101734936,\n",
       "  0.123534694,\n",
       "  0.18195859,\n",
       "  0.13236128,\n",
       "  0.10895658,\n",
       "  0.08684993,\n",
       "  0.038610984,\n",
       "  0.13654324,\n",
       "  0.14530161,\n",
       "  0.08678888,\n",
       "  0.07113357,\n",
       "  0.06659075,\n",
       "  0.06577128,\n",
       "  0.07307454,\n",
       "  0.11011176,\n",
       "  0.102798104,\n",
       "  0.11846322,\n",
       "  0.0780984,\n",
       "  0.07124324,\n",
       "  0.10842186,\n",
       "  0.061963454,\n",
       "  0.06286737,\n",
       "  0.059831824,\n",
       "  0.04538658,\n",
       "  0.10713065,\n",
       "  0.11622175,\n",
       "  0.073340975,\n",
       "  0.113141015,\n",
       "  0.12442717,\n",
       "  0.14991955,\n",
       "  0.11165273,\n",
       "  0.06426655,\n",
       "  0.07966876,\n",
       "  0.1768517,\n",
       "  0.054490518,\n",
       "  0.11508926,\n",
       "  0.08209375,\n",
       "  0.079413846,\n",
       "  0.16821757,\n",
       "  0.11083657,\n",
       "  0.14948528,\n",
       "  0.14797358,\n",
       "  0.12607124,\n",
       "  0.06349912,\n",
       "  0.066951394,\n",
       "  0.08448972,\n",
       "  0.09706676,\n",
       "  0.09320906,\n",
       "  0.075834356,\n",
       "  0.06032389,\n",
       "  0.095046714,\n",
       "  0.06620751,\n",
       "  0.03697459,\n",
       "  0.08387049,\n",
       "  0.10604237,\n",
       "  0.17634018,\n",
       "  0.08754602,\n",
       "  0.1141198,\n",
       "  0.12917796,\n",
       "  0.04681152,\n",
       "  0.038882475,\n",
       "  0.04736846,\n",
       "  0.09190145,\n",
       "  0.13611585,\n",
       "  0.06663236,\n",
       "  0.1290676,\n",
       "  0.10088913,\n",
       "  0.14011467,\n",
       "  0.07693212,\n",
       "  0.10894266,\n",
       "  0.123545855,\n",
       "  0.08917025,\n",
       "  0.09193606,\n",
       "  0.09924901,\n",
       "  0.21730673,\n",
       "  0.08981862,\n",
       "  0.054752134,\n",
       "  0.051652897,\n",
       "  0.075248025,\n",
       "  0.04850611,\n",
       "  0.1488519,\n",
       "  0.12308453,\n",
       "  0.07248013,\n",
       "  0.10061576,\n",
       "  0.1226559,\n",
       "  0.054098338,\n",
       "  0.10911799,\n",
       "  0.0972791,\n",
       "  0.12602341,\n",
       "  0.041235037,\n",
       "  0.08213884,\n",
       "  0.07988429,\n",
       "  0.0527199,\n",
       "  0.060433514,\n",
       "  0.057525832,\n",
       "  0.10237385,\n",
       "  0.08185969,\n",
       "  0.081969485,\n",
       "  0.07138981,\n",
       "  0.10067341,\n",
       "  0.11225735,\n",
       "  0.06614919,\n",
       "  0.13265997,\n",
       "  0.09187747,\n",
       "  0.19154313,\n",
       "  0.10809483,\n",
       "  0.07625735,\n",
       "  0.1219596,\n",
       "  0.107954316,\n",
       "  0.1394152,\n",
       "  0.13026829,\n",
       "  0.06394394,\n",
       "  0.14907917,\n",
       "  0.08717443,\n",
       "  0.093143255,\n",
       "  0.07539823,\n",
       "  0.06471358,\n",
       "  0.07056474,\n",
       "  0.10741024,\n",
       "  0.1317218,\n",
       "  0.072665446,\n",
       "  0.15386209,\n",
       "  0.0651843,\n",
       "  0.07333173,\n",
       "  0.10887812,\n",
       "  0.0918107,\n",
       "  0.082766555,\n",
       "  0.062038958,\n",
       "  0.08639118,\n",
       "  0.08334033,\n",
       "  0.04770865,\n",
       "  0.08631625,\n",
       "  0.049513865,\n",
       "  0.05880873,\n",
       "  0.048535053,\n",
       "  0.098728865,\n",
       "  0.08817535,\n",
       "  0.0959266,\n",
       "  0.057061166,\n",
       "  0.08711106,\n",
       "  0.11812494,\n",
       "  0.062358074,\n",
       "  0.08271613,\n",
       "  0.09199363,\n",
       "  0.082499325,\n",
       "  0.073130205,\n",
       "  0.10943623,\n",
       "  0.06534583,\n",
       "  0.102512464,\n",
       "  0.17037627,\n",
       "  0.058797225,\n",
       "  0.05061857,\n",
       "  0.05422135,\n",
       "  0.09835382,\n",
       "  0.0864391,\n",
       "  0.07214558,\n",
       "  0.10287186,\n",
       "  0.10948106,\n",
       "  0.05631168,\n",
       "  0.21270528,\n",
       "  0.034866285,\n",
       "  0.041006118,\n",
       "  0.054599285,\n",
       "  0.03405512,\n",
       "  0.0645297,\n",
       "  0.078886256,\n",
       "  0.093783215,\n",
       "  0.12601255,\n",
       "  0.14544673,\n",
       "  0.095099315,\n",
       "  0.085337095,\n",
       "  0.09041287,\n",
       "  0.1203461,\n",
       "  0.093445346,\n",
       "  0.18166,\n",
       "  0.078469634,\n",
       "  0.093059115,\n",
       "  0.071045354,\n",
       "  0.12255278,\n",
       "  0.11142747,\n",
       "  0.06096839,\n",
       "  0.12374042,\n",
       "  0.085278004,\n",
       "  0.06377347,\n",
       "  0.041205894,\n",
       "  0.14726955,\n",
       "  0.061312053,\n",
       "  0.05257657,\n",
       "  0.045259118,\n",
       "  0.08779697,\n",
       "  0.0676031,\n",
       "  0.09593227,\n",
       "  0.101985306,\n",
       "  0.08564596,\n",
       "  0.030777862,\n",
       "  0.052240938,\n",
       "  0.0749066,\n",
       "  0.116319895,\n",
       "  0.07920632,\n",
       "  0.04206947,\n",
       "  0.05830576,\n",
       "  0.07059596,\n",
       "  0.119332135,\n",
       "  0.0706476,\n",
       "  0.097113535,\n",
       "  0.042124256,\n",
       "  0.089604564,\n",
       "  0.04392904,\n",
       "  0.119816914,\n",
       "  0.13599673,\n",
       "  0.08612565,\n",
       "  0.094768986,\n",
       "  0.07175349,\n",
       "  0.08942767,\n",
       "  0.082387924,\n",
       "  0.045054942,\n",
       "  0.070698366,\n",
       "  0.036117077,\n",
       "  0.1230492,\n",
       "  0.09995759,\n",
       "  0.07694559,\n",
       "  0.0514336,\n",
       "  0.07265115,\n",
       "  0.052016817,\n",
       "  0.10330679,\n",
       "  0.048961118,\n",
       "  0.05995812,\n",
       "  0.103354156,\n",
       "  0.11494388,\n",
       "  0.063394435,\n",
       "  0.049073517,\n",
       "  0.052807763,\n",
       "  0.06436673,\n",
       "  0.06577309,\n",
       "  0.043808144,\n",
       "  0.08417012,\n",
       "  0.07716445,\n",
       "  0.08471971,\n",
       "  0.076064855,\n",
       "  0.07539439,\n",
       "  0.11358984,\n",
       "  0.104535356,\n",
       "  0.05070652,\n",
       "  0.09024872,\n",
       "  0.05691586,\n",
       "  0.07843359,\n",
       "  0.06019053,\n",
       "  0.08374995,\n",
       "  0.08080277,\n",
       "  0.053938717,\n",
       "  0.11831018,\n",
       "  0.13380727,\n",
       "  0.057219766,\n",
       "  0.053545907,\n",
       "  0.08495176,\n",
       "  ...])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
